{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy\n",
    "from math import log, exp, lgamma, sqrt\n",
    "import scipy.stats\n",
    "from pylab import *\n",
    "\n",
    "LOG_FACTORIAL_CACHE = [lgamma(n + 1) for n in range(1000)]\n",
    "def log_factorial(n):\n",
    "    return LOG_FACTORIAL_CACHE[n]\n",
    "\n",
    "ALT_MINOR = 0\n",
    "REF_MINOR = 1\n",
    "OUTLIER = 2\n",
    "POSSIBLE_HIDDEN_VALUES = [ALT_MINOR, REF_MINOR, OUTLIER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#given unnormalized log-probabilities[c*log p1, c*log p2, c*log p3 . . . c*log pN] return normalized\n",
    "## probabilities [p1, p2. ..pN] in an underflow-aware way\n",
    "def probs_from_log_probs(log_probs):\n",
    "    M = max(log_probs)\n",
    "    unnormalized_probs = [exp(p - M) for p in log_probs]\n",
    "    normalizer = sum(unnormalized_probs)\n",
    "    return [p / normalizer for p in unnormalized_probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#An auxiliary function (in Java, a private static) that is the log of the factor obtained from marginalizing\n",
    "#out a single bias ratio lambda\n",
    "def phi(f, alpha, beta, a, r):\n",
    "    if f == 1: #if f == 1 the conditional is exactly gamma already and can be marginalized immediately\n",
    "        rho = alpha + r\n",
    "        tau = beta\n",
    "        log_c = 0\n",
    "    else:\n",
    "        n = a + r\n",
    "        w = (1-f)*(a - alpha + 1) + beta*f #intermediate for calculating the mode of the conditional\n",
    "        mode = (sqrt(w**2 + 4*beta*f*(1-f)*(alpha + r -1)) - w)/(2*beta*(1-f))\n",
    "        curvature = (alpha + r -1)/(mode**2) - n*(1-f)**2/(f + (1-f)*mode)**2\n",
    "\n",
    "        #match the mode, curvature, and normalization to an effective gamma conditional\n",
    "        tau = mode*curvature\n",
    "        rho = mode*tau + 1\n",
    "        log_c = (r + alpha - rho)*log(mode) + (tau - beta)*mode - n * log(f + (1-f)*mode)\n",
    "    \n",
    "    return log_c + lgamma(rho) - rho*log(tau)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine the marginalization factor with f- and pi-dependent factors to get the log-likelihood\n",
    "#contribution from a single site.  In Java this could be  member function but in Python\n",
    "#all the self. gets messy and tedious\n",
    "def log_likelihood_original_parameters(f, alpha, beta, a, r, pi):\n",
    "        log_alt_minor_marginalization = phi(f, alpha, beta, a, r)\n",
    "        log_ref_minor_marginalization = phi(1-f, alpha, beta, a, r)\n",
    "        log_outlier_marginalization = lgamma(alpha) - alpha * log(beta)\n",
    "        \n",
    "        log_alt_minor_term = log_alt_minor_marginalization + log(1-pi) + a * log(f) + r * log(1-f)\n",
    "        log_ref_minor_term = log_ref_minor_marginalization + log(1-pi) + r * log(f) + a * log(1-f)\n",
    "        log_outlier_term = log_outlier_marginalization + log(2*pi) + log_factorial(a) + log_factorial(r) - log_factorial(a+r+1)\n",
    "        return log_sum_log(log_alt_minor_term, log_ref_minor_term, log_outlier_term) + alpha*log(beta) - lgamma(alpha)\n",
    "    \n",
    "def log_likelihood(f, mu, beta, a, r, pi):\n",
    "    return log_likelihood_original_parameters(f, mu*beta, beta, a, r, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#overflow- and underflow-savvy log(exp(a)+exp(b)+exp(c))\n",
    "#log(exp(a)+exp(b)+exp(c)) = log[exp(M)*(exp(a-M)+exp(b-M)+exp(c-M))] where  M = max(a,b,c)\n",
    "#                          = M + log[(exp(a-M)+exp(b-M)+exp(c-M))]\n",
    "def log_sum_log(a,b,c):\n",
    "    M = max(a,b,c)\n",
    "    return M + log(exp(a-M)+exp(b-M)+exp(c-M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AlleleFractionModel:\n",
    "    \n",
    "    def __init__(self, alt_counts, ref_counts, segment_lengths):\n",
    "        self.a = alt_counts\n",
    "        self.r = ref_counts\n",
    "        self.N = len(alt_counts)\n",
    "        self.S = len(segment_lengths)\n",
    "        \n",
    "        segment_ends = numpy.cumsum(segment_lengths)\n",
    "        segment_starts = [0]\n",
    "        segment_starts.extend(segment_ends[:-1])\n",
    "        \n",
    "        self.het_to_segment = [0 for n in range(self.N)]\n",
    "        self.segment_ranges = []\n",
    "        for s, start_end in enumerate(zip(segment_starts, segment_ends)):\n",
    "            start, end = start_end\n",
    "            for j in range(start, end):\n",
    "                self.het_to_segment[j] = s\n",
    "            self.segment_ranges.append(range(start, end))\n",
    "        \n",
    "        self.pi = 0.01 \n",
    "        self.mu, self.beta = 1, 10\n",
    "        \n",
    "        #initialize f naively\n",
    "        self.f = [0.5 for s in range(self.S)]\n",
    "        for s in range(self.S):\n",
    "            total_minor, total_major = 0, 0\n",
    "            for h in self.segment_ranges[s]:\n",
    "                total_minor += min(self.a[h], self.r[h])\n",
    "                total_major += max(self.a[h], self.r[h])\n",
    "            self.f[s] = min(1, (total_minor + 1) / (total_major + total_minor + 2))\n",
    "        \n",
    "        #set up the adaptive metropolis samplers\n",
    "        self.f_sampler = [AdaptiveMetropolisSampler(0.01, 0, 1) for s in range(self.S)]\n",
    "        self.mu_sampler = AdaptiveMetropolisSampler(0.01, 0)\n",
    "        self.beta_sampler = AdaptiveMetropolisSampler(10,0,1000)\n",
    "        self.pi_sampler = AdaptiveMetropolisSampler(0.1, 0, 0.1)\n",
    "        \n",
    "    def get_f(self, h):\n",
    "        return self.f[self.het_to_segment[h]]\n",
    "        \n",
    "    def update_pi(self):\n",
    "        self.pi = self.pi_sampler.sample(self.pi, lambda pi: self.log_conditional_on_pi(pi))\n",
    "    \n",
    "    def update_f(self, s):\n",
    "        self.f[s] = self.f_sampler[s].sample(self.f[s], lambda f: self.log_conditional_on_f(f, s))\n",
    "    \n",
    "    def update_mu(self):\n",
    "        self.mu = self.mu_sampler.sample(self.mu, lambda mu: self.log_conditional_on_mu(mu))\n",
    "    \n",
    "    def update_beta(self):\n",
    "        self.beta = self.beta_sampler.sample(self.beta, lambda beta: self.log_conditional_on_beta(beta))\n",
    "    \n",
    "    #fix everything except pi\n",
    "    def log_conditional_on_pi(self, pi):\n",
    "        return sum( log_likelihood(self.get_f(h), self.mu, self.beta, self.a[h], self.r[h], pi) for h in range(self.N))\n",
    "    \n",
    "    #fix everything except mu\n",
    "    def log_conditional_on_mu(self, mu):\n",
    "        return sum( log_likelihood(self.get_f(h), mu, self.beta, self.a[h], self.r[h], self.pi) for h in range(self.N))\n",
    "    \n",
    "    #fix everything except beta\n",
    "    def log_conditional_on_beta(self, beta):\n",
    "        return sum( log_likelihood(self.get_f(h), self.mu, beta, self.a[h], self.r[h], self.pi) for h in range(self.N))\n",
    "    \n",
    "    #fix everything except f[s]\n",
    "    def log_conditional_on_f(self, f, s):\n",
    "        return sum( log_likelihood(f, self.mu, self.beta, self.a[h], self.r[h], self.pi) for h in self.segment_ranges[s])  \n",
    "    \n",
    "    #site h's contribution to the log-likelihood\n",
    "    def log_likelihood_contribution(self, h):\n",
    "        log_alt_minor_marginalization = phi(self.get_f(h), self.alpha, self.beta, self.a[h], self.r[h])\n",
    "        log_ref_minor_marginalization = phi(1 - self.get_f(h), self.alpha, self.beta, self.a[h], self.r[h])\n",
    "        log_outlier_marginalization = lgamma(self.alpha) - self.alpha * log(self.beta)\n",
    "        \n",
    "        log_alt_minor_term = log_alt_minor_marginalization + log(1-self.pi)\n",
    "        log_ref_minor_term = log_alt_minor_marginalization\n",
    "        log_outlier_term = log_alt_minor_marginalization\n",
    "            \n",
    "    def mcmc_iteration(self):\n",
    "        for s in range(self.S):\n",
    "            self.update_f(s)\n",
    "        self.update_mu()\n",
    "        self.update_beta()\n",
    "        self.update_pi()\n",
    "        \n",
    "    def report(self):\n",
    "        print(\"Mu, beta, pi: \", self.mu, self.beta, self.pi)\n",
    "        \n",
    "    def mcmc(self, num_burn_in, num_iterations):\n",
    "        for i in range(num_burn_in):\n",
    "            print(\"Iteration %d out of %d burn-in iterations\" % (i, num_burn_in))\n",
    "            self.mcmc_iteration()\n",
    "            self.report()\n",
    "            \n",
    "        f_samples = []\n",
    "        mu_samples = []\n",
    "        beta_samples = []\n",
    "        pi_samples = []\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            print(\"Iteration %d out of %d iterations\" % (i, num_iterations))\n",
    "            self.mcmc_iteration()\n",
    "            self.report()\n",
    "            f_samples.append(self.f.copy())\n",
    "            pi_samples.append(self.pi)\n",
    "            mu_samples.append(self.mu)\n",
    "            beta_samples.append(self.beta)\n",
    "        \n",
    "        return f_samples, pi_samples, mu_samples, beta_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdaptiveMetropolisSampler:\n",
    "    '''\n",
    "    This sampler makes symmetric Metropolis proposals x_proposed = x_current + delta_x,\n",
    "    where delta_x ~ Cauchy(width). We choose a Cauchy because of its fat tails.  \n",
    "    \n",
    "    Idea: take the proposal as a lambda\n",
    "    \n",
    "    right now I assume sampling a positive quanity, hence take the absolute value of the proposal\n",
    "    (this is symmetric).\n",
    "    \n",
    "    The width is tuned to achieve a desired acceptance rate via the Robbins-Monro algorithm.\n",
    "    In order to be define a valid MCMC algorithm the amount of tuning at each step must go to zero.  \n",
    "    This sampler achieves this by scaling correction at the nth iteration by a/(b+n)\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, width=0.1, lower=float(\"-inf\"), upper=float(\"inf\")):\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        self.a = 20\n",
    "        self.b = 20\n",
    "        self.width = width\n",
    "        self.optimal_acceptance_rate = 0.4\n",
    "        self.n = 1\n",
    "    \n",
    "    ##sample given a current value and a lambda to compute the log probability\n",
    "    def sample(self, x_old, log_probability):\n",
    "        x_new = x_old + scipy.stats.cauchy.rvs()*self.width\n",
    "        \n",
    "        if x_new < self.lower or x_new > self.upper:\n",
    "            acceptance_probability = 0.0\n",
    "        else:\n",
    "            acceptance_probability = min(1, exp(log_probability(x_new) - log_probability(x_old)))\n",
    "            \n",
    "        correction_factor = (acceptance_probability - self.optimal_acceptance_rate) * self.a / (self.b + self.n)\n",
    "        self.width *= math.exp(correction_factor)\n",
    "        self.n += 1\n",
    "        \n",
    "        return x_new if scipy.stats.uniform.rvs() < acceptance_probability else x_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##generate inputs to MLE_solution using same generative model as our inference\n",
    "def generate_data(average_hets_in_segment, num_segments, average_depth, alpha, beta, pi):\n",
    "    MIN_HETS_PER_SEGMENT = 3\n",
    "    a = []\n",
    "    r = []\n",
    "    \n",
    "    start_het = [0 for s in range(num_segments)]\n",
    "    end_het = [0 for s in range(num_segments)]\n",
    "    segments = []\n",
    "    f = []\n",
    "    num_alt_minor = 0\n",
    "    num_ref_minor = 0\n",
    "    \n",
    "    biases = []\n",
    "    h = 0\n",
    "    for s in range(num_segments):\n",
    "        start_het[s] = h\n",
    "        minor_fraction = scipy.stats.uniform.rvs(0, 0.5)\n",
    "        f.append(minor_fraction)\n",
    "        num_hets = MIN_HETS_PER_SEGMENT + scipy.stats.poisson.rvs(mu=average_hets_in_segment)\n",
    "        for j in range(num_hets):\n",
    "            segments.append(s)\n",
    "            bias = scipy.stats.gamma.rvs(alpha, scale=1/beta)\n",
    "            biases.append(bias)\n",
    "            \n",
    "            is_outlier = scipy.stats.bernoulli.rvs(pi)\n",
    "            if is_outlier:\n",
    "                alt_prob = scipy.stats.uniform.rvs()\n",
    "            else:\n",
    "                indicator = scipy.stats.bernoulli.rvs(0.5)\n",
    "\n",
    "                if indicator == ALT_MINOR:\n",
    "                    num_alt_minor += 1\n",
    "                    f_alt = minor_fraction\n",
    "                if indicator == REF_MINOR:\n",
    "                    num_ref_minor += 1\n",
    "                    f_alt = 1 - minor_fraction\n",
    "\n",
    "                alt_prob = f_alt/(f_alt + (1-f_alt)*bias)\n",
    "                \n",
    "            total_count = 1 + scipy.stats.poisson.rvs(mu=average_depth)\n",
    "            a.append(scipy.stats.binom.rvs(n=total_count, p = alt_prob))\n",
    "            r.append(total_count - a[h])\n",
    "            h+=1\n",
    "            \n",
    "        end_het[s] = h\n",
    "        segment_lengths = [e - s for e, s in zip(end_het, start_het)]\n",
    "    return a, r, segment_lengths, f, biases\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(average_hets_in_segment, num_segments, average_depth, mu, beta, pi, num_iterations, burn_in):\n",
    "    alpha = mu*beta\n",
    "    a, r, segment_lengths, true_f, true_biases = generate_data(average_hets_in_segment, num_segments, average_depth, alpha, beta, pi)\n",
    "    true_mean_bias = numpy.mean(true_biases)\n",
    "    #print(true_f)\n",
    "    model = AlleleFractionModel(a, r, segment_lengths)\n",
    "    f_samples, pi_samples, mu_samples, beta_samples = model.mcmc(burn_in, num_iterations)\n",
    "    \n",
    "    print(\"actual mean bias, posterior mean bias,  standard deviation of mean bias\")\n",
    "    print(\"%8.3f %8.3f %8.3f\" % ( mu, numpy.mean(mu_samples), numpy.std(mu_samples)))\n",
    "    \n",
    "    print(\"actual inverse overdispersion, posterior inverse overdispersion,  standard deviation of inverse overdispersion\")\n",
    "    print(\"%8.3f %8.3f %8.3f\" % ( beta, numpy.mean(beta_samples), numpy.std(beta_samples)))\n",
    "                              \n",
    "    print(\"actual outlier robability, posterior mean outlier probability,  standard deviation of outlier probability\")\n",
    "    print(\"%8.3f %8.3f %8.3f\" % ( pi, numpy.mean(pi_samples), numpy.std(pi_samples)))\n",
    "\n",
    "    print(\"The true BAF, estimated BAF (mean of MCMC samples), and error bars of estimate (standard deviation of MCMC samples) are:\")\n",
    "    print(\"True      Estimate  Error_Bars\")\n",
    "    \n",
    "    for s in range(len(true_f)):\n",
    "        f = [sample[s] for sample in f_samples]\n",
    "        f_minor = [x if x < 0.5 else (1 - x) for x in f]\n",
    "        print(\"%8.3f %8.3f %8.3f\" % (true_f[s], numpy.mean(f_minor), numpy.std(f_minor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test(average_hets_in_segment=20, num_segments=500, average_depth=40, mu=1.09, beta = 120, pi=0.03, num_iterations=200, burn_in=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
